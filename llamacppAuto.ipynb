{"cells":[{"cell_type":"markdown","source":["Скрипт сделал pawsfpv"],"metadata":{"id":"y1oDaqjopmub"}},{"cell_type":"markdown","metadata":{"id":"m36IcBCYFV6N"},"source":["# Подготовка llama.cpp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JaLD7LELm3EB"},"outputs":[],"source":["# @title  {\"display-mode\":\"form\"}\n","\n","from google.colab import drive\n","\n","#@markdown ___\n","#@markdown ## Собрать llama.cpp из исходников\n","#@markdown Сохранит программу на гугл диск и будет использовать её для быстрого включения.\n","#@markdown Первое включение очень долгое, сборка занимает до получаса\n","build = False #@param {type:\"boolean\"}\n","#@markdown ___\n","#@markdown ## Вариант сборки llama.cpp (cpu или cuda)\n","#@markdown Для CUDA сменить среду выполнения на T4, иначе сменить на CPU.\n","\n","#@markdown Важно правильно переключить среду выполнения для первых включений в соответствии с галочкой ниже, при неправильной среде выполнения потребуется пересборка программы\n","cuda = False # @param {\"type\":\"boolean\"}\n","#@markdown Сохранение всего набора программ\n","save_full = False #@param {type:\"boolean\"}\n","#@markdown ___\n","#@markdown ## Пересборка llama.cpp\n","#@markdown Это удалит уже собранную llama.cpp с гугл диска, если она была. Будет повторная сборка, как при первом включении\n","rebuild = False #@param {type:\"boolean\"}\n","\n","#@markdown ___\n","#@markdown #Или скачать готовую по ссылке (когда build выключено)\n","link_ready = \"https://github.com/p-fpv/llamacpp_colab/releases/download/ver1/llamaCUDA.zip\" #@param {type:\"string\"}\n","\n","\n","!rm -rf ./build\n","!rm -rf ./llamaCUDA.zip\n","!rm -rf ./llamaCPU.zip\n","\n","if build == 1:\n","  drive.mount('/content/drive')\n","  import os.path\n","\n","  if cuda:\n","    if rebuild:\n","      !rm -rf /content/drive/MyDrive/llamaCUDA.zip\n","\n","    file_path1 = \"/content/drive/MyDrive/llamaCUDA.zip\"\n","    if os.path.exists(file_path1):\n","      !rsync -arh --info=progress2 /content/drive/MyDrive/llamaCUDA.zip /content/llamaCUDA.zip\n","      !7z x llama*.zip -y\n","    else:\n","      !rm -rf /content/llama.cpp\n","      !apt install ccache\n","      !git clone https://github.com/ggml-org/llama.cpp\n","      #cuda\n","      !cd /content/llama.cpp/;cmake -B build -DGGML_CUDA=ON -DBUILD_SHARED_LIBS=OFF;cmake --build build --config Release -j 2\n","      !mkdir /content/build/\n","      if save_full:\n","        !rsync -arh --info=progress2 /content/llama.cpp/build/bin /content/build/\n","      else:\n","        !mkdir /content/build/bin/\n","        !rsync -arh --info=progress2 /content/llama.cpp/build/bin/llama-server /content/build/bin/\n","      !7z a llamaCUDA.zip /content/build\n","      !rsync -arh --info=progress2 /content/llamaCUDA.zip /content/drive/MyDrive/llamaCUDA.zip\n","\n","  else:\n","    if rebuild:\n","      !rm -rf /content/drive/MyDrive/llamaCPU.zip\n","\n","    file_path2 = \"/content/drive/MyDrive/llamaCPU.zip\"\n","    if os.path.exists(file_path2):\n","      !rsync -arh --info=progress2 /content/drive/MyDrive/llamaCPU.zip /content/llamaCPU.zip\n","      !7z x llama*.zip -y\n","    else:\n","      !rm -rf /content/llama.cpp\n","      !git clone https://github.com/ggml-org/llama.cpp\n","      #CPU\n","      !cd /content/llama.cpp/;cmake -B build -DGGML_CUDA=OFF -DBUILD_SHARED_LIBS=OFF;cmake --build build --config Release -j 2\n","      !mkdir /content/build/\n","      if save_full:\n","        !rsync -arh --info=progress2 /content/llama.cpp/build/bin /content/build/\n","      else:\n","        !mkdir /content/build/bin/\n","        !rsync -arh --info=progress2 /content/llama.cpp/build/bin/llama-server /content/build/bin/\n","      !7z a llamaCPU.zip /content/build\n","      !rsync -arh  --info=progress2 /content/llamaCPU.zip /content/drive/MyDrive/llamaCPU.zip\n","\n","elif build == 0:\n","  !rm -rf ./build\n","  !wget $link_ready\n","  !7z x llama*.zip\n","\n","from IPython.display import clear_output\n","clear_output()\n","print(\"Ok\")\n","#@markdown ---"]},{"cell_type":"markdown","metadata":{"id":"3Mqv1iNMFL33"},"source":["# Model Download"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3luLiY8mQSvX"},"outputs":[],"source":["# @title  {\"display-mode\":\"form\"}\n","#@markdown  ___\n","use_model_link = \"2\" # @param [\"1\", \"2\", \"3\", \"4\", \"5\"]\n","#@markdown  ___\n","\n","link1 = \"https://huggingface.co/IlyaGusev/saiga_yandexgpt_8b_gguf/resolve/main/saiga_yandexgpt_8b.Q8_0.gguf\" #@param {type:\"string\"}\n","link2 = \"https://huggingface.co/IlyaGusev/saiga_yandexgpt_8b_gguf/resolve/main/saiga_yandexgpt_8b.Q6_K.gguf\" #@param {type:\"string\"}\n","link3 = \"https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q6_K_L.gguf\" #@param {type:\"string\"}\n","link4 = \"https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_M.gguf\" #@param {type:\"string\"}\n","link5 = \"https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q8_0.gguf\" #@param {type:\"string\"}\n","#@markdown  ___\n","if use_model_link == \"1\":\n","  link = link1\n","elif use_model_link == \"2\":\n","  link = link2\n","elif use_model_link == \"3\":\n","  link = link3\n","elif use_model_link == \"4\":\n","  link = link4\n","elif use_model_link == \"5\":\n","  link = link5\n","print(link)\n","!rm -rf ./*.gguf\n","!wget $link\n","\n","# https://huggingface.co/IlyaGusev/saiga_yandexgpt_8b_gguf\n","from IPython.display import clear_output\n","clear_output()\n","print(\"Ok\")"]},{"cell_type":"markdown","metadata":{"id":"rHGBXzCFFBDb"},"source":["# Включить llama-server"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QHeUzqaMWRc3"},"outputs":[],"source":["# @title  {\"display-mode\":\"form\"}\n","!pip install pyngrok\n","from IPython.display import clear_output\n","clear_output()\n","\n","import getpass\n","from pyngrok import ngrok, conf\n","ngrok.kill()\n","!sleep 1\n","#@markdown ___\n","Ngrok_token = \"\" #@param {type:\"string\"}\n","#@markdown  https://dashboard.ngrok.com/get-started/your-authtoken\n","\n","#@markdown ___\n","\n","#srv=ngrok.connect(7860, pyngrok_config=conf.PyngrokConfig(auth_token=\"\") , bind_tls=True).public_url\n","port = \"5000\"\n","\n","# conf.get_default().auth_token = getpass.getpass()\n","public_url = ngrok.connect(port, pyngrok_config=conf.PyngrokConfig(auth_token=Ngrok_token)).public_url\n","# print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"\")\n","print(f\" * ngrok tunnel \\\"{public_url}\\\"\")\n","\n","cnt = 32768 #@param {type:\"number\"}\n","ngl = 100\n","# apikey = \"\" #@param {type:\"string\"}\n","\n","#@markdown  ___\n","logs = False #@param {type:\"boolean\"}\n","\n","if logs:\n","  !cd /content/build/bin/; chmod +x ./*; ./llama-server \\\n","  -m /content/*.gguf \\\n","  --host 0.0.0.0 --port $port \\\n","  -c $cnt -fa -t 2 --mlock --no-mmap -ngl $ngl --parallel 1 #--api-key apikey # --parallel 1 --no-warmup\n","else:\n","  !cd /content/build/bin/; chmod +x ./*; ./llama-server \\\n","  -m /content/*.gguf \\\n","  --host 0.0.0.0 --port $port \\\n","  -c $cnt -fa -t 2 --mlock --no-mmap -ngl $ngl --parallel 1 2>/dev/null\n","\n","# from IPython.display import clear_output\n","logs_cls = True #@param {type:\"boolean\"}\n","#@markdown  ___\n","if logs_cls:\n","  clear_output()\n","print(\"Ok\")"]}],"metadata":{"colab":{"collapsed_sections":["m36IcBCYFV6N","3Mqv1iNMFL33","rHGBXzCFFBDb"],"provenance":[{"file_id":"1rxG4CYtsX6YEzMXoy7qdrDAsTI9FmQoT","timestamp":1742731703835}],"gpuType":"T4","authorship_tag":"ABX9TyPugdwgP4uqNQv3+aXzPKcX"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}